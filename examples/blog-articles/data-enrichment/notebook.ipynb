{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Enrichment: A Complete Guide to Enhancing Your Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's data-driven business landscape, organizations face a critical challenge: transforming raw data into actionable insights. Data enrichment has emerged as a fundamental process that enables businesses to enhance their existing datasets with additional, valuable information from multiple sources.\n",
    "\n",
    "This comprehensive guide explores data enrichment â€” what it is, why it matters, and how organizations can implement it effectively. Whether you're looking to improve customer profiles, enhance marketing campaigns, or strengthen decision-making processes, understanding data enrichment is crucial for maintaining a competitive edge in 2025 and beyond.\n",
    "\n",
    "As data sources continue to multiply and diversify, businesses need robust tools and strategies to collect, integrate, and enhance their data efficiently. Modern solutions, including AI-powered platforms like [Firecrawl](https://firecrawl.dev), are transforming how organizations approach data enrichment by automating collection processes and ensuring data quality at scale.\n",
    "\n",
    "Throughout this guide, we'll explore:\n",
    "- Essential concepts and methodologies in data enrichment\n",
    "- Tools and technologies that streamline the enrichment process\n",
    "- Best practices for implementing data enrichment strategies\n",
    "- Real-world applications and success stories\n",
    "- Common challenges and their solutions\n",
    "\n",
    "Whether you're new to data enrichment or looking to optimize your existing processes, this guide provides the insights and practical knowledge needed to enhance your data quality and drive better business outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Enrichment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data enrichment is the process of enhancing, refining, and augmenting existing data by combining it with relevant information from multiple sources. This process transforms basic data points into comprehensive, actionable insights that drive better business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding data enrichment tools\n",
    "\n",
    "Modern data enrichment tools automate the complex process of collecting and integrating data from diverse sources. These tools typically offer:\n",
    "\n",
    "- Automated data collection from websites, APIs, and databases\n",
    "- Data validation and cleaning capabilities\n",
    "- Schema mapping and transformation features\n",
    "- Integration with existing business systems\n",
    "- Quality assurance and monitoring\n",
    "\n",
    "For example, web scraping engines like Firecrawl enable organizations to automatically structured web data through AI-powered extraction, eliminating the need for manual data gathering while ensuring consistency and accuracy. We will see a complete Firecrawl data enrichment workflow later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of data enrichment services\n",
    "\n",
    "Data enrichment services generally fall into six main categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/enrichment_categories.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Demographic**\n",
    "   - Adding personal information like age, income, education\n",
    "   - Enhancing geographic data with location-specific insights\n",
    "   - Including professional and career-related information\n",
    "\n",
    "2. **Behavioral**\n",
    "   - Incorporating customer interaction history\n",
    "   - Adding purchase patterns and preferences\n",
    "   - Including digital footprint data like website visits and engagement\n",
    "\n",
    "3. **Company**\n",
    "   - Adding firmographic data (company size, revenue, industry)\n",
    "   - Including technological stack information\n",
    "   - Incorporating business relationships and hierarchies\n",
    "\n",
    "4. **Psychographic**\n",
    "   - Adding personality traits and characteristics\n",
    "   - Including personal values and attitudes\n",
    "   - Incorporating lifestyle choices and interests\n",
    "   - Adding motivation and decision-making patterns\n",
    "\n",
    "5. **Geographic**\n",
    "   - Location intelligence and spatial analysis\n",
    "   - Regional market data and demographics\n",
    "   - Geographic business opportunities\n",
    "   - Local economic indicators\n",
    "   - Competitive landscape by region\n",
    "\n",
    "6. **Contact Data**\n",
    "   - Email addresses and phone numbers\n",
    "   - Social media profiles\n",
    "   - Professional contact details\n",
    "   - Business communication preferences\n",
    "   - Contact verification and validation\n",
    "\n",
    "These different types of data enrichment services can be combined and integrated to create comprehensive data profiles. For example, combining demographic and behavioral data provides deeper customer insights, while merging company and geographic data enables better B2B market analysis. The key is selecting the right combination of enrichment types based on your specific business needs and objectives.\n",
    "\n",
    "As we move into examining B2B data enrichment specifically, we'll see how these various enrichment types come together to support business decision-making and growth strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2B data enrichment fundamentals\n",
    "\n",
    "B2B data enrichment focuses specifically on enhancing business-related information to improve lead generation, account-based marketing, and sales processes. Key aspects include:\n",
    "\n",
    "- Company verification and validation\n",
    "- Decision-maker identification and contact information \n",
    "- Industry classification and market positioning\n",
    "- Technology stack and tool usage data\n",
    "- Company financial health indicators (e.g., revenue ranges $1-10M, $10-50M, etc.)\n",
    "- Organizational structure and hierarchy details\n",
    "- Employee count brackets (e.g., 1-50, 51-200, 201-1000)\n",
    "\n",
    "The effectiveness of B2B data enrichment relies heavily on:\n",
    "- Data accuracy and freshness through diverse data sources\n",
    "- Real-time enrichment for time-sensitive data vs. batch processing for static data\n",
    "- Compliance with data protection regulations\n",
    "- Integration with existing CRM and sales tools\n",
    "- Scalability of enrichment processes\n",
    "- Cost-effectiveness and validation across multiple data providers\n",
    "\n",
    "These fundamentals form the foundation for successful B2B data enrichment initiatives. When implemented effectively, they enable organizations to make more informed business decisions and develop targeted strategies for growth.\n",
    "\n",
    "Let's explore the various tools and services available for data enrichment and how to select the right ones for your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment Tools and Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data enrichment tools come in various forms, each serving specific use cases and industries. Here are some of the most effective solutions:\n",
    "\n",
    "1. Web scraping and data collection tools\n",
    "\n",
    "- [Firecrawl](https://firecrawl.dev): AI-powered web scraping for structured data extraction ($0-100/mo)\n",
    "- [Bright Data](https://brightdata.com): Enterprise-grade data collection infrastructure with proxy networks ($500+/mo)\n",
    "- [Scrapy](https://scrapy.org/): Open-source Python framework for custom scraping solutions (Free). Key features: Custom extraction rules, proxy rotation, scheduling\n",
    "\n",
    "2. Business intelligence platforms  \n",
    "\n",
    "- [ZoomInfo](https://www.zoominfo.com/): Company and contact information enrichment ($15k+/year). Integrates with: [Salesforce](https://salesforce.com), [HubSpot](https://hubspot.com), Marketo\n",
    "- [Clearbit](https://clearbit.com/): Real-time company and person data API ($99-999/mo). Unique feature: Logo and domain matching\n",
    "- [InsideView](https://www.demandbase.com/): Market intelligence and company insights ($500+/mo). Specializes in: Sales intelligence and prospecting\n",
    "\n",
    "3. Customer data platforms (CDPs)\n",
    "\n",
    "- [Segment](https://segment.com/): Customer data collection and integration ($120+/mo). Key differentiator: 300+ pre-built integrations\n",
    "- [Tealium](https://www.tealium.com): Real-time customer data orchestration (Custom pricing). Unique feature: Machine learning predictions\n",
    "- [mParticle](https://www.mparticle.com/): Customer data infrastructure ($500+/mo). Best for: Mobile app analytics\n",
    "\n",
    "4. Data validation and cleansing\n",
    "\n",
    "- [Melissa Data](https://www.melissa.com/): Address verification and standardization ($500+/mo)\n",
    "- [DemandTools](https://www.validity.com/demandtools/): Salesforce data cleansing ($999+/year)\n",
    "- [Informatica Data Quality](https://www.informatica.com/): Enterprise data quality management (Custom pricing)\n",
    "\n",
    "These tools provide a foundation for data enrichment, but selecting the right service requires careful evaluation of several key factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right data enrichment solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When selecting a data enrichment service, organizations must evaluate several critical factors. Data quality metrics serve as the foundation, with accuracy rates ideally exceeding 95% and response times under 200ms. Services should maintain 99.9% uptime SLAs and provide comprehensive data validation methods.\n",
    "\n",
    "Integration capabilities determine implementation success. Services should offer well-documented REST APIs, support JSON/CSV formats, and connect with major CRM platforms. The ability to handle both real-time and batch processing (up to 100k records/hour) provides essential flexibility.\n",
    "\n",
    "Cost evaluation requires examining API pricing tiers, volume discounts, and hidden fees. Consider vendor reputation, support quality (24/7 availability), and compliance certifications (GDPR, CCPA, SOC 2). Regular security audits and data handling practices should meet industry standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom data enrichment solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom data enrichment solutions provide organizations with tailored approaches to data enhancement. Web scraping platforms like Firecrawl and Selenium enable automated extraction of public data, while custom APIs facilitate direct integration with specialized data sources. Python frameworks such as Selenium, BeautifulSoup and Pandas streamline data processing and transformation workflows. Common pitfalls include rate limiting issues, unstable selectors, and data quality inconsistencies.\n",
    "\n",
    "These solutions often incorporate modular architectures for flexibility. Organizations can combine multiple data sources, implement custom validation rules, and design specific enrichment pipelines. Advanced features include proxy rotation for reliable scraping, rate limiting for API compliance, and parallel processing for improved performance. Key challenges include maintaining data consistency across sources and handling API deprecation gracefully.\n",
    "\n",
    "Development of custom solutions requires careful consideration of scalability and maintenance. Teams should implement robust error handling, comprehensive logging, and automated testing. Documentation and version control ensure long-term sustainability, while modular design enables iterative improvements and feature additions. Consider trade-offs between custom development costs versus off-the-shelf solutions - custom solutions typically require 2-3x more initial investment but offer greater control and flexibility.\n",
    "\n",
    "For a complete example of data enrichment process in Python, you can read a later section on using Firecrawl for enriching football match statistics dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product data enrichment solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product data enrichment solutions transform basic product information into comprehensive, market-ready datasets. These tools excel at automated attribute extraction, leveraging advanced technologies to identify and categorize product features like dimensions, materials, and industry certifications. Image recognition capabilities enhance product listings with accurate visual data, while competitive pricing analysis ensures market alignment with 95%+ accuracy rates.\n",
    "\n",
    "Modern product enrichment platforms support bulk processing across multiple languages at speeds of 50,000+ products per hour, making them ideal for global operations. They often incorporate industry-specific taxonomies to maintain standardization and enable rich media enhancement for improved product presentation. Key integration capabilities include seamless connections with [Shopify](https://shopify.com), [WooCommerce](woocommerce.com), and other major e-commerce platforms, along with built-in data validation methods for ensuring attribute accuracy and completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer data enrichment solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer data enrichment platforms serve as central hubs for creating comprehensive customer profiles, achieving match rates of 85-95% for B2B data and 70-85% for B2C data. At their core, these platforms excel at identity resolution, connecting disparate data points to form unified customer views. They incorporate behavioral analytics to understand customer actions and automatically append demographic information to existing profiles, with leading platforms like Clearbit and ZoomInfo offering 98%+ accuracy rates.\n",
    "\n",
    "Integration features form the backbone of these platforms, with real-time API access enabling immediate data updates at speeds of 10-20ms per record. Webhook support facilitates automated workflows, while custom field mapping ensures compatibility with existing systems. Sophisticated data synchronization maintains consistency across all connected platforms, with enterprise solutions like [FullContact](https://fullcontact.com) processing up to 1M records per day.\n",
    "\n",
    "Security and compliance remain paramount in customer data enrichment. Modern platforms incorporate robust GDPR compliance measures and granular data privacy controls, typically costing $0.05-0.15 per enriched record at scale. They maintain detailed audit trails and provide comprehensive consent management systems to protect both organizations and their customers.\n",
    "\n",
    "The key to successful data enrichment lies in selecting tools that align with your specific use cases while maintaining data quality and compliance standards. Best practices include refreshing enriched data every 3-6 months and implementing data quality monitoring. Organizations should start with a pilot program using one or two tools before scaling to a full implementation, with typical ROI ranging from 3-5x for marketing use cases and 5-8x for sales applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2B Data Enrichment: A Complete Guide\n",
    "\n",
    "B2B data enrichment has become increasingly critical for organizations seeking to enhance their business intelligence and decision-making capabilities. Let's explore the key tools and implementation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2B data enrichment tools\n",
    "\n",
    "Enterprise solutions form the backbone of large-scale B2B data operations, offering comprehensive coverage and high accuracy. These platforms excel at providing deep company insights and market intelligence:\n",
    "\n",
    "1. Enterprise solutions\n",
    "\n",
    "- [ZoomInfo](https://zoominfo.com): Industry leader with 95%+ accuracy for company data\n",
    "- [D&B Hoovers](https://dnb.com): Comprehensive business intelligence with global coverage\n",
    "- [InsideView](https://insideview.com): Real-time company insights and market intelligence\n",
    "\n",
    "API-first platforms enable seamless integration into existing workflows, providing real-time data enrichment capabilities for automated systems:\n",
    "\n",
    "2. API-First platforms\n",
    "\n",
    "- [Clearbit enrichment API](clearbit.com): Real-time company data lookup\n",
    "- [FullContact API](https://docs.fullcontact.com/): Professional contact verification\n",
    "- [Hunter.io](https://hunter.io): Email verification and discovery\n",
    "\n",
    "Data validation tools ensure data quality and compliance, critical for maintaining accurate business records:\n",
    "\n",
    "3. Data Validation Tools\n",
    "- [Melissa Data](https://melissa.com): Address and contact verification\n",
    "- [Neverbounce](https://neverbounce.com): Email validation services\n",
    "- [DueDil](duedill.com): Company verification and compliance\n",
    "\n",
    "Typical pricing models range from:\n",
    "- Pay-per-lookup: $0.05-0.25 per record\n",
    "- Monthly subscriptions: $500-5000/month\n",
    "- Enterprise contracts: $50,000+ annually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices for B2B data\n",
    "\n",
    "1. Data quality management\n",
    "\n",
    "Regular validation and monitoring are essential for maintaining high-quality B2B data. Implement email format validation using standard regex patterns and schedule bi-monthly data refreshes. Track match rates with a target of >85% accuracy through systematic sampling.\n",
    "\n",
    "2. Integration strategy\n",
    "\n",
    "Build a robust integration framework using standardized JSON for API communications. Critical components include:\n",
    "- Real-time alert systems for failed enrichments\n",
    "- Retry logic with exponential backoff\n",
    "- Comprehensive request logging\n",
    "- Cost monitoring and caching mechanisms\n",
    "\n",
    "3. Compliance and security\n",
    "\n",
    "Ensure compliance with data protection regulations through proper documentation and security measures. Implement TLS 1.3 for data transfer and AES-256 for storage, while maintaining regular security audits and access reviews.\n",
    "\n",
    "4. Data Scalability\n",
    "\n",
    "Design systems to handle large-scale data processing efficiently. Focus on data partitioning for datasets exceeding 1M records and implement auto-scaling capabilities to manage processing spikes. Monitor key performance metrics to maintain system health.\n",
    "\n",
    "5. ROI Measurement\n",
    "\n",
    "Track the business impact of your enrichment efforts by measuring:\n",
    "- Cost vs. revenue impact\n",
    "- Conversion rate improvements\n",
    "- Lead qualification efficiency\n",
    "- Sales cycle optimization\n",
    "\n",
    "Key Performance Indicators:\n",
    "- Match rate: >85%\n",
    "- Data accuracy: >95%\n",
    "- Enrichment coverage: >90%\n",
    "- Time to value: <48 hours\n",
    "\n",
    "Let's put these best practices into action by exploring a real-world example of data enrichment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Enrichment in Python: Amazon Data Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a practical example of data enrichment using Python with [Amazon listings data from 2020](https://www.kaggle.com/datasets/promptcloud/amazon-product-dataset-2020). This Kaggle-hosted dataset provides an excellent opportunity to demonstrate how to enrich outdated product information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Amazon listings dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should download the dataset stored as a ZIP file on Kaggle servers. These commands pull that zip file and extract the CSV file inside in your current working directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Download the zip file\n",
    "curl -L -o amazon-product-dataset-2020.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/promptcloud/amazon-product-dataset-2020\n",
    "unzip amazon-product-dataset-2020.zip\n",
    "\n",
    "# Move the desired file and rename it\n",
    "mv home/sdf/marketing_sample_for_amazon_com-ecommerce__20200101_20200131__10k_data.csv amazon_listings.csv\n",
    "\n",
    "# Delete unnecessary files\n",
    "rm -rf home amazon-product-dataset-2020.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's install a few dependencies we are going to use:\n",
    "\n",
    "```bash\n",
    "pip instal pandas firecrawl-py pydantic python-dotenv\n",
    "```\n",
    "\n",
    "Now, we can load the dataset with Pandas to explore it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/amazon_listings.csv\")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10k products with 28 attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Uniq Id', 'Product Name', 'Brand Name', 'Asin', 'Category',\n",
       "       'Upc Ean Code', 'List Price', 'Selling Price', 'Quantity',\n",
       "       'Model Number', 'About Product', 'Product Specification',\n",
       "       'Technical Details', 'Shipping Weight', 'Product Dimensions', 'Image',\n",
       "       'Variants', 'Sku', 'Product Url', 'Stock', 'Product Details',\n",
       "       'Dimensions', 'Color', 'Ingredients', 'Direction To Use',\n",
       "       'Is Amazon Seller', 'Size Quantity Variant', 'Product Description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ecommerce listing datasets, missing values are the #1 problem. Let's see if that's true of this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Description      1.000000\n",
       "Sku                      1.000000\n",
       "Brand Name               1.000000\n",
       "Asin                     1.000000\n",
       "Size Quantity Variant    1.000000\n",
       "List Price               1.000000\n",
       "Direction To Use         1.000000\n",
       "Quantity                 1.000000\n",
       "Ingredients              1.000000\n",
       "Color                    1.000000\n",
       "Dimensions               1.000000\n",
       "Product Details          1.000000\n",
       "Stock                    1.000000\n",
       "Upc Ean Code             0.996601\n",
       "Product Dimensions       0.952110\n",
       "Variants                 0.752250\n",
       "Model Number             0.177165\n",
       "Product Specification    0.163167\n",
       "Shipping Weight          0.113777\n",
       "Category                 0.082983\n",
       "Technical Details        0.078984\n",
       "About Product            0.027295\n",
       "Selling Price            0.010698\n",
       "Image                    0.000000\n",
       "Product Name             0.000000\n",
       "Product Url              0.000000\n",
       "Is Amazon Seller         0.000000\n",
       "Uniq Id                  0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_percentages = df.isnull().sum() / df.shape[0]\n",
    "\n",
    "null_percentages.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output, we can see that almost 15 attributes are missing for all products while some have partially incomplete data. Only five attributes are fully present:\n",
    "\n",
    "- Unique ID\n",
    "- Is Amazon Seller\n",
    "- Product URL\n",
    "- Product Name\n",
    "- Image\n",
    "\n",
    "Even when details are present, we can't count on them since they were recorded in 2020 and have probably changed. So, our goal is to enrich this data with updated information as well as filling in the missing columns as best as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching Amazon listings data with Firecrawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've explored the dataset and identified missing information, let's use Firecrawl to enrich our Amazon product data. We'll create a schema that defines what information we want to extract from each product URL.\n",
    "\n",
    "First, let's import the required libraries and initialize Firecrawl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "app = FirecrawlApp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a Pydantic model that describes the product information we want to extract. This schema helps Firecrawl's AI understand what data to look for on each product page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product(BaseModel):\n",
    "    name: str = Field(description=\"The name of the product\")\n",
    "    image: str = Field(description=\"The URL of the product image\")\n",
    "    brand: str = Field(description=\"The seller or brand of the product\")\n",
    "    category: str = Field(description=\"The category of the product\")\n",
    "    price: float = Field(description=\"The current price of the product\")\n",
    "    rating: float = Field(description=\"The rating of the product\")\n",
    "    reviews: int = Field(description=\"The number of reviews of the product\")\n",
    "    description: str = Field(description=\"The description of the product written below its price.\")\n",
    "    dimensions: str = Field(description=\"The dimensions of the product written below its technical details.\")\n",
    "    weight: str = Field(description=\"The weight of the product written below its technical details.\")\n",
    "    in_stock: bool = Field(description=\"Whether the product is in stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, let's take the last 100 product URLs from our dataset and enrich them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last 100 URLs\n",
    "urls = df['Product Url'].tolist()[-100:]\n",
    "\n",
    "# Start batch scraping job\n",
    "batch_scrape_data = app.batch_scrape_urls(urls, params={\n",
    "    \"formats\": [\"extract\"],\n",
    "    \"extract\": {\"schema\": Product.model_json_schema()}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results of the batch-scraping job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped: 84 products\n",
      "Failed to scrape: 16 products\n"
     ]
    }
   ],
   "source": [
    "# Separate successful and failed scrapes\n",
    "failed_items = [\n",
    "    item for item in batch_scrape_data[\"data\"] if item[\"metadata\"][\"statusCode\"] != 200\n",
    "]\n",
    "success_items = [\n",
    "    item for item in batch_scrape_data[\"data\"] if item[\"metadata\"][\"statusCode\"] == 200\n",
    "]\n",
    "\n",
    "print(f\"Successfully scraped: {len(success_items)} products\")\n",
    "print(f\"Failed to scrape: {len(failed_items)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a successful enrichment result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "    'extract': {\n",
    "        'name': 'Leffler Home Kids Chair, Red',\n",
    "        'brand': 'Leffler Home',\n",
    "        'image': 'https://m.media-amazon.com/images/I/71IcFQJ8n4L._AC_SX355_.jpg',\n",
    "        'price': 0,\n",
    "        'rating': 0,\n",
    "        'weight': '26 Pounds',\n",
    "        'reviews': 0,\n",
    "        'category': \"Kids' Furniture\",\n",
    "        'in_stock': False,\n",
    "        'dimensions': '20\"D x 24\"W x 21\"H',\n",
    "        'description': 'Every child deserves their own all mine chair...'\n",
    "    },\n",
    "    'metadata': {\n",
    "        'url': 'https://www.amazon.com/Leffler-Home-14000-21-63-01-Kids-Chair/dp/B0784BXPC2',\n",
    "        'statusCode': 200,\n",
    "        # ... other metadata\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations about the enrichment results:\n",
    "\n",
    "1. **Success Rate**: Out of 100 URLs, we successfully enriched 84 products. The 16 failures were mostly due to products no longer being available on Amazon (404 errors).\n",
    "\n",
    "2. **Data Quality**: For successful scrapes, we obtained all desired details, including:\n",
    "\n",
    "- Complete product dimensions and weights\n",
    "- Updated category information\n",
    "- Current availability status\n",
    "- High-resolution image URLs\n",
    "- Detailed product descriptions\n",
    "\n",
    "3. **Missing Values**: Some numeric fields (price, rating, reviews) returned 0 for unavailable products, which we should handle in our data cleaning step.\n",
    "\n",
    "For large-scale enrichment, Firecrawl also offers an asynchronous API that can handle thousands of URLs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Start async batch job\n",
    "batch_job = app.async_batch_scrape_urls(\n",
    "    urls,\n",
    "    params={\"formats\": [\"extract\"], \"extract\": {\"schema\": Product.model_json_schema()}},\n",
    ")\n",
    "\n",
    "# Check job status until it finishes\n",
    "job_status = app.check_batch_scrape_status(batch_job[\"id\"])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is particularly useful when enriching the entire dataset of 10,000+ products, as it allows you to monitor progress and handle results in chunks rather than waiting for all URLs to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll explore how to merge this enriched data back into our original dataset and handle any discrepancies between old and new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What more can you do with Firecrawl?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our Amazon product enrichment example demonstrated basic data extraction, AI-powered scraping offers several advantages for broader data enrichment scenarios:\n",
    "\n",
    "1. __Natural language data extraction__\n",
    "\n",
    "Instead of relying on brittle HTML selectors or XPath expressions, you can describe the data you want to extract in plain English. This approach works across different website layouts and remains stable even when sites update their structure. For example, you could request \"find all pricing tables that show enterprise plan features\" or \"extract author biographies from the bottom of articles\" without specifying exact HTML locations. All these features are packed into the `scape_url` method of Firecrawl and we have [a full guide on how best to use it](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint).\n",
    "\n",
    "2. __Recursive website crawling__\n",
    "\n",
    "Beyond scraping individual URLs, you can automatically discover and process entire website sections. The crawler understands site structure semantically, following relevant links while avoiding navigation menus, footers, and other non-content areas. This is particularly useful when enriching data from documentation sites, knowledge bases, or product catalogs. You can learn how this crawling process works in [our separate crawl endpoint guide](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl).\n",
    "\n",
    "3. __Multiple output formats__\n",
    "\n",
    "The same content can be extracted in different formats depending on your needs:\n",
    "\n",
    "- Structured data for database storage (like we extracted earlier)\n",
    "- Markdown for documentation\n",
    "- Plain text for natural language processing\n",
    "- HTML for web archives\n",
    "- Screenshots for visual records\n",
    "\n",
    "This flexibility eliminates the need for additional conversion steps in your data pipeline.\n",
    "\n",
    "4. __Intelligent content processing__\n",
    "\n",
    "The AI-powered approach helps solve common web scraping challenges:\n",
    "\n",
    "- Automatically detecting and extracting data from tables, lists, and other structured elements\n",
    "- Understanding content relationships and hierarchies\n",
    "- Handling dynamic JavaScript-rendered content\n",
    "- Maintaining context across multiple pages\n",
    "- Filtering out irrelevant content like ads and popups\n",
    "\n",
    "5. __Integration capabilities__\n",
    "\n",
    "The extracted data can feed directly into modern data and AI workflows:\n",
    "\n",
    "- Vector databases for semantic search\n",
    "- Large language models for analysis\n",
    "- Business intelligence tools for reporting\n",
    "- Machine learning pipelines for training data\n",
    "- Monitoring systems for change detection\n",
    "\n",
    "The key advantage across all these use cases is reduced maintenance overhead. Traditional scrapers require constant updates as websites change, while semantic extraction remains stable. This makes it particularly suitable for long-running data enrichment projects where reliability and consistency are crucial.\n",
    "\n",
    "In the next section, we'll conclude with key takeaways and implementation steps for successful data enrichment projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Data enrichment has become essential for organizations seeking to maintain competitive advantage through high-quality, comprehensive datasets. Through this guide, we've explored how modern tools and techniques can streamline the enrichment process.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Start with clear data quality objectives and metrics\n",
    "- Choose enrichment tools based on your specific use case and scale\n",
    "- Implement proper validation and error handling\n",
    "- Consider both batch and real-time enrichment approaches\n",
    "- Maintain data freshness through regular updates\n",
    "\n",
    "### Implementation Steps\n",
    "\n",
    "- Audit your current data to identify gaps\n",
    "- Select appropriate enrichment sources and tools\n",
    "- Build a scalable enrichment pipeline\n",
    "- Implement quality monitoring\n",
    "- Schedule regular data refresh cycles\n",
    "\n",
    "### Additional Resources\n",
    "- [Firecrawl Documentation](https://docs.firecrawl.dev/)\n",
    "- [Data Quality Best Practices](https://www.datacommunitydc.org/blog/2013/08/data-quality-best-practices)\n",
    "- [Web Scraping Ethics Guide](https://www.zyte.com/learn/web-scraping-ethics/)\n",
    "\n",
    "By following these guidelines and leveraging modern tools, organizations can build robust data enrichment pipelines that provide lasting value for their data-driven initiatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
